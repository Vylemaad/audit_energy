{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0bd0fcb-fbcd-4f51-ab34-49651228a51b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "## <center> Projet 3 - Parcours Machine Learning :<br/>Consommation électrique de la ville de Seattle </center>\n",
    "\n",
    "<p style=\"text-align:center;\">Vous travaillez pour la ville de Seattle. Pour atteindre son objectif de ville neutre en émissions de carbone en 2050, votre équipe s’intéresse de près aux émissions des bâtiments non destinés à l’habitation.</p>\n",
    "\n",
    "<center><img src=\"image/logo_seattle.png\" alt=\"logo_seattle\" width=\"500\" /></center>\n",
    "\n",
    "##### Les données\n",
    "\n",
    "Les données de consommation sont [à télécharger ici](https://www.kaggle.com/city-of-seattle/sea-building-energy-benchmarking#2015-building-energy-benchmarking.csv).\n",
    "\n",
    "##### Problématique de la ville de Seattle\n",
    "\n",
    "<p style=\"text-align:justify;\">Des relevés minutieux ont été effectués par vos agents en 2015 et en 2016. Cependant, ces relevés sont coûteux à obtenir, et à partir de ceux déjà réalisés, vous voulez tenter de prédire les émissions de CO2 et la consommation totale d’énergie de bâtiments pour lesquels elles n’ont pas encore été mesurées.\n",
    "\n",
    "Votre prédiction se basera sur les données déclaratives du permis d'exploitation commerciale (taille et usage des bâtiments, mention de travaux récents, date de construction..)\n",
    "\n",
    "Vous cherchez également à évaluer l’intérêt de l’\"ENERGY STAR Score\" pour la prédiction d’émissions, qui est fastidieux à calculer avec l’approche utilisée actuellement par votre équipe.</p>\n",
    "\n",
    "##### Votre mission\n",
    "\n",
    "Vous sortez tout juste d’une réunion de brief avec votre équipe. Voici un récapitulatif de votre mission :\n",
    "\n",
    "- Réaliser une courte analyse exploratoire.\n",
    "- Tester différents modèles de prédiction afin de répondre au mieux à la problématique.\n",
    "\n",
    "Avant de quitter la salle de brief, Douglas, le project lead, vous donne quelques pistes, et erreurs à éviter :\n",
    "\n",
    ">L’objectif est de te passer des relevés de consommation annuels (attention à la fuite de données), mais rien ne t'interdit d’en déduire des variables plus simples (nature et proportions des sources d’énergie utilisées). \n",
    ">\n",
    ">Fais bien attention au traitement des différentes variables, à la fois pour trouver de nouvelles informations (peut-on déduire des choses intéressantes d’une simple adresse ?) et optimiser les performances en appliquant des transformations simples aux variables (normalisation, passage au log, etc.).\n",
    ">\n",
    ">Mets en place une évaluation rigoureuse des performances de la régression, et optimise les hyperparamètres et le choix d’algorithme de ML à l’aide d’une validation croisée.\n",
    ">\n",
    "\n",
    "#### Let's go :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "45debcd4-6c97-43c2-9dfb-f8c2fda34488",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "###################################################################################################\n",
    "####################                       Import librairies                    ###################\n",
    "###################################################################################################\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "import numpy as np\n",
    "from math import *\n",
    "from numpy import isnan\n",
    "from global_functions_variables import *\n",
    "import altair as alt\n",
    "from vega_datasets import data\n",
    "import folium\n",
    "import folium.plugins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760bca56-d736-4eba-a214-727beae7e178",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "#### Les fichiers CSV :\n",
    "\n",
    "<p>Les relevés énergétiques sont stockés dans 2 fichiers csv :</p>\n",
    "\n",
    "- Les relevés de 2015\n",
    "- Les relevés de 2016\n",
    "\n",
    "<p>On remarque rapidement que les deux bases de données n'ont pas le même nombre de colonnes.\n",
    "Une analyse des différences montre les faits suivants :</p>\n",
    "\n",
    "1. Les variables de localisation du fichier de 2016 :<br>\n",
    "    *Latitude*,<br>\n",
    "    *Longitude*,<br>\n",
    "    *City*,<br>\n",
    "    *Address*,<br>\n",
    "    *zipCode*,<br>\n",
    "    *State*<br> sont **compressées** dans une seule variable *Location* dans le fichier de 2015\n",
    "\n",
    "2. Les variables du fichier 2015 :<br> \n",
    "     *OtherFuelUse(kBtu)*,<br> \n",
    "     *2010 Census Tracts*,<br> \n",
    "     *Seattle Police Department Micro Community Policing Plan Areas*,<br> \n",
    "     *City Council Districts*,<br> \n",
    "     *SPD Beats*,<br>\n",
    "     *Zip Codes*,<br>non présentes en 2016, sont **vides, faussées ou sans rapport avec la consommation d'énergie**\n",
    "    \n",
    "3. Les dernières différences de variables représentent **les mêmes données sous la même métrique** mais **sous un autre nom**.\n",
    "\n",
    "<p>Pour rassembler ces deux bases de données il faut donc :</p>\n",
    "\n",
    "1. Décompresser la variable *Location* de 2015 et sauvegarder ses valeurs dans de nouvelles colonnes de même noms que la version 2016\n",
    "\n",
    "2. Retirer les colonnes non pertinentes du DataFrame de 2015\n",
    "\n",
    "3. Renommer les variables restantes pour s'adapter aux variables de 2016\n",
    "\n",
    "Une fois ces étapes effectuées, les deux DataFrames pourront être combinés afin de traiter les données de 2015 et de 2016 simultanément."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0fe56cce-67c4-499b-a70e-acd304f8fc88",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 2016 CSV file had 46 fields and the 2015 CSV file had 47 fields\n",
      "Both datasets have the same number of columns, it could be concatenate now\n",
      "The final shape of 2015-2016 dataset is (6716, 46)\n"
     ]
    }
   ],
   "source": [
    "###################################################################################################\n",
    "####################          Load and merge CSV files from 2015 and 2016       ###################\n",
    "###################################################################################################\n",
    "\n",
    "#read csv file and stock it in dataframe\n",
    "data_2015 = pd.read_csv(r'Data/2015-building-energy-benchmarking.csv',\n",
    "                        sep=',', encoding='utf-8', na_values='')\n",
    "data_2016 = pd.read_csv(r'Data/2016-building-energy-benchmarking.csv',\n",
    "                        sep=',', encoding='utf-8', na_values='')\n",
    "print(f\"The 2016 CSV file had {data_2016.shape[1]}\\\n",
    " fields and the 2015 CSV file had {data_2015.shape[1]} fields\")\n",
    "    \n",
    "# 1.Re-Organize location variables - Dezip latitude, longitude ect..\n",
    "decompressed = {'Latitude': [],\n",
    "                'Longitude' : [],\n",
    "                'City' : [],\n",
    "                'Address' : [],\n",
    "                'ZipCode' : [],\n",
    "                'State': []}\n",
    "\n",
    "for loc in data_2015['Location']:\n",
    "    loc_dict = eval(loc)\n",
    "    address_dict = eval(loc_dict['human_address'])\n",
    "    decompressed['Latitude'].append(round(float(loc_dict['latitude']),5))\n",
    "    decompressed['Longitude'].append(round(float(loc_dict['longitude']),5))\n",
    "    decompressed['City'].append(address_dict['city'])\n",
    "    decompressed['Address'].append(address_dict['address'])\n",
    "    decompressed['ZipCode'].append(address_dict['zip'])\n",
    "    decompressed['State'].append(address_dict['state'])\n",
    "data_2015 = pd.concat([data_2015.drop('Location',axis=1), pd.DataFrame(decompressed)],axis=1)\n",
    "\n",
    "# 2. Remove variables only present in 2015 AND empty, irrelevant or wrong :\n",
    "data_2015.drop([\"OtherFuelUse(kBtu)\",'2010 Census Tracts',\n",
    "                'Seattle Police Department Micro Community Policing Plan Areas',\n",
    "                'City Council Districts',\"SPD Beats\", 'Zip Codes'], axis=1, inplace=True)\n",
    "\n",
    "# For \"GHGEmissions(MetricTonsCO2e) --> TotalGHGEmissions \n",
    "#we have checked beforehand that 2015 and 2016 dataset use the same metrics !\n",
    "\n",
    "# 3. Rename 2015 variable to their 2016 names\n",
    "data_2015.rename(columns={\"Comment\" : \"Comments\",\n",
    "                          \"GHGEmissions(MetricTonsCO2e)\" : \"TotalGHGEmissions\",\n",
    "                         \"GHGEmissionsIntensity(kgCO2e/ft2)\" : \"GHGEmissionsIntensity\"\n",
    "                         },inplace=True)\n",
    "\n",
    "# Finally, combine both into a 2015-2016 dataframe\n",
    "if len(set(data_2016.columns).symmetric_difference(set(data_2015.columns))) == 0 :\n",
    "    print(\"Both datasets have the same number of columns, it could be concatenate now\")\n",
    "    data_global = pd.concat([data_2015, data_2016], axis=0)\n",
    "else:\n",
    "    print(\"Something gone wrong\")\n",
    "\n",
    "data_global.reset_index(drop=True, inplace=True)\n",
    "print(f\"The final shape of 2015-2016 dataset is {data_global.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "30dec608-134e-485c-bfcf-ecb3ef4826b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data_global\n",
    "#Reformate values spell check PrimaryPropertyType values - remove '/n' at the end\n",
    "type_names = df['PrimaryPropertyType'].apply(lambda x : x.replace('\\n',''))\n",
    "df.loc[:, 'PrimaryPropertyType'] = type_names \n",
    "\n",
    "df['LargestPropertyUseType'].fillna(df['PrimaryPropertyType'], inplace=True)\n",
    "df.loc[(df['LargestPropertyUseTypeGFA'].isna() & df['SecondLargestPropertyUseType'].isna()),\n",
    "       'LargestPropertyUseTypeGFA'] = df['PropertyGFABuilding(s)']\n",
    "df['LargestPropertyUseType'].replace(['Mixed Use Property'], ['Other'] ,inplace=True)\n",
    "df['ListOfAllPropertyUseTypes'].fillna(df['LargestPropertyUseType'],inplace=True)\n",
    "df['LargestPropertyUseType'].replace(rename_usetype.keys(), rename_usetype.values(), inplace=True)\n",
    "df['SecondLargestPropertyUseType'].replace(rename_usetype.keys(), rename_usetype.values(), inplace=True)\n",
    "df['ThirdLargestPropertyUseType'].replace(rename_usetype.keys(), rename_usetype.values(), inplace=True)\n",
    "df['PrimaryPropertyType'].replace(rename_usetype.keys(), rename_usetype.values(), inplace=True)\n",
    "#Zones géographique categorie\n",
    "neighbor_names = df['Neighborhood'].apply(lambda x : x.upper())\n",
    "df.loc[:, 'Neighborhood'] = neighbor_names\n",
    "\n",
    "addresses = df['Address'].apply(lambda x : x.upper())\n",
    "df.loc[:, 'Address'] = addresses\n",
    "addresses = df['PropertyName'].apply(lambda x : x.upper())\n",
    "df.loc[:, 'PropertyName'] = addresses\n",
    "df['Neighborhood'].replace('DELRIDGE NEIGHBORHOODS',\n",
    "                                    'DELRIDGE', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90eaa82a-d6c1-4636-acd6-c9870dbb5962",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Analyse des variables:\n",
    "\n",
    "<p>Notre DataFrame possède 46 variables qu'il faut étudier pour déduire de leur pertinence et de leur impact sur notre projet. Pour cela, observer leur type, leur valeurs uniques, leur valeurs manquantes ou encore leur métrique apporte de précieuses informations.</p> \n",
    "\n",
    "#### Les variables de localisation:\n",
    "\n",
    "<p>Les données de localisation sont importantes pour visualiser nos résultats sur une carte, ou étudier la consommation par quartier. L'emplacement d'un bâtiment est-il corrélé à sa consommation énergétique? Pour le découvrir, il faut conserver ces données. </p>\n",
    "\n",
    "- *Latitude* et *Longitude* : <br/> Axe Nord-Sud et Ouest-Est des coordonnées terrestres, permet une localisation précise et pratiquement unique. Utilisable pour détecter les doublons.<br/>**à conserver**\n",
    "\n",
    "- *Neighborhood* et *CouncilDistrictCode* : <br/> Chacune de ces variables représente des zones géographiques, en les combinant, la granularité sera plus fine. Les valeurs de *Neighborhood* ne sont pas standardisées (orthographe alternative etc...). Regroupement sous une seule variable à envisager.<br/>**à conserver, standardiser et reformater**\n",
    "\n",
    "- *Address* :<br/> Représentation \"humaine\" de la position du bâtiment, ne sera pas utilisable par nos systèmes, d'autant plus que de meilleures informations de positionnement existent. Utilisable pour l'identification.<br/> **à reformater dans une variable d'identification**\n",
    "\n",
    "- *State*, *ZipCode* et *City* :<br/>  *State* et *City* n'ont qu'une seule valeur possible dans ce jeu de données. *ZipCode* n'est pas pertinent.<br/>**à retirer**\n",
    "\n",
    "\n",
    "#### Les variables de description du bâtiment:\n",
    "\n",
    "<p>Rappel du projet :</p>\n",
    "\n",
    ">Votre prédiction se basera sur les données déclaratives du permis d'exploitation commerciale (taille et usage des bâtiments, mention de travaux récents, date de construction..)\n",
    "\n",
    "<p>Les informations relatives aux bâtiments sont les données de base de notre système, sur lesquels on s'appuieras pour prédire la consommation. La récupération de ces données est peu coûteuse.</p> \n",
    "\n",
    "- *OSEBuildingID*, *TaxParcelIdentificationNumber* et *PropertyName* :<br/>Variables d'identification, identifient les bâtiments ( ou leur propriétaire dans le cas de *TaxParcelIdentificationNumber*) par un numéro ou un nom. Utiles pour manipuler les données et détecter les doublons <br/>**à reformater dans une variable d'identification**\n",
    "\n",
    "- *YearBuilt*, *NumberofBuildings* et *NumberofFloors* :<br/>Variables descriptives du bâtiments.<br/>**à conserver**\n",
    "\n",
    "- *PropertyGFATotal*, *PropertyGFAParking*, *PropertyGFABuilding(s)* :<br/>GFA signifie Gross Floor Area : la superficie au sol du bâtiment en incluant les murs extérieurs. Ces variables représentent la superficie au sol du bâtiment avec ou sans prendre en compte le parking. Une forte corrélation est attendue entre ces variables, l'analyse exploratoire les réduira. Presque 50% des individus n'ont qu'un seul type d'usage.<br/> **à conserver**\n",
    "\n",
    "- *BuildingType*, *PrimaryPropertyType*, *ListOfAllPropertyUseTypes* et autres :<br/>Type d'usage du bâtiment selon différentes granularités. Si le bâtiment a plus d'un seul type d'usage, la surface au sol allouée aux trois premiers types d'usage est disponible dans les variables *LargestPropertyUseTypeGFA*, *SecondLargestPropertyUseTypeGFA* et *ThirdLargestPropertyUseTypeGFA*. Cette méthode de stockage d'information provoque un fort taux de valeurs manquantes. De plus, une redondance est présente entre *ListOfAllPropertyUseTypes* et les trois variables *LargestPropertyUseType*, *SecondLargestPropertyUseType*, *ThirdLargestPropertyUseType*. Des modifications sur ces variables sont donc à prévoir.<br/>**à reformater et conserver** sauf *ListOfAllPropertyUseTypes* **à retirer pour redondance**\n",
    "\n",
    "#### Les variables énergetiques:\n",
    "\n",
    "<p>Rappel du projet :</p>\n",
    "\n",
    ">Cependant, ces relevés sont coûteux à obtenir, et à partir de ceux déjà réalisés, vous voulez tenter de prédire les émissions de CO2 et la consommation totale d’énergie de bâtiments pour lesquels elles n’ont pas encore été mesurées.\n",
    "\n",
    "<p>Les variables énergetiques sont les données que notre modélisation doit prédire. Le recensement de ses données est couteux et à éviter. Les données conservées de cette catégorie servirons de vérification lors de l'entrainement de nos modèle supervisés</p> \n",
    "       \n",
    "- *YearsENERGYSTARCertified* et *ENERGYSTARScore*: <br/> Un des objectifs du projet est de déterminer la pertinence du score ENERGYSTAR. Pour autant *YearsENERGYSTARCertified* a un fort taux de valeurs manquantes et est redondants avec le score<br/> *ENERGYSTARScore* **à conserver**, *YearsENERGYSTARCertified* **à retirer**\n",
    "       \n",
    "- *SteamUse(kBtu)*, *NaturalGas(kBtu)*, *Electricity(kBtu)* et *SiteEnergyUse(kBtu)*: <br/> Les mesures des différents types d'énergie et leur total en unité thermique britannique. Notre modèle n'a pas pour but de déterminer quel type d'énergie est consommée, seul le total est conservé<br/> *SiteEnergyUse(kBtu)* **à conserver**, les autres **à retirer**       \n",
    "\n",
    "- *NaturalGas(therms)* et *Electricity(kWh)*: <br/> Ces variables sont des doublons et leur métrique n'est pas le standard dans notre base de données<br/> **à retirer**\n",
    "\n",
    "- *TotalGHGEmissions*: <br/> Le modèle doit également prédire l'émissions de gaz à effet de serre, la variable est conserver pour l'apprentissage<br/> **à conserver**\n",
    "       \n",
    "- *SiteEUI(kBtu/sf)*, *GHGEmissionsIntensity* et *SourceEUI(kBtu/sf)*: <br/> Ces variables sont le résultat d'un calcul entre la surface au sol du bâtiment et sa consommation. Elles découlent directement de la variable à prédire<br/> **à retirer**\n",
    "       \n",
    "- *SiteEUIWN(kBtu/sf)*, *SourceEUIWN(kBtu/sf)* et *SiteEnergyUseWN(kBtu)*: <br/> WN : Weather Normalized : Il s'agit des variables précédentes normalisée par les données météorologiques des 30 dernières années. Notre base de données ne possède pas d'informations relatives à la météo ou au climat, pourtant il s'agit d'une donnée essentielle en prédiction de consommation énergétique. Prédire une consommation relative au climat est selon moi bien plus pertinent que de prédire la consommation brute en supposant une météo stable. Pour autant, pour suivre l'intitulé du projet, ces informations (dont la formule est inconnue) ne seront pas utilisées <br/> **à retirer**\n",
    "       \n",
    "<p>Un énorme biais se remarque par la non prise en compte de variable temporelle. La consommation électrique d'une année sur l'autre diffèrent due à de nombreux facteurs qui sont mis de côté dans notre étude. Le résultat finale s'en retrouvera affaiblie et nos prédictions seront inexactes. Cela est d'autant plus d'actualité que le changement climatique a profondement bouleversés la stabilité météorologique qui est supposé dans ce modèle erroné.</p>\n",
    "       \n",
    "#### Autres variables :\n",
    "\n",
    "- *Outlier* et *Comments*: <br/> Ces variables ont un taux de valeurs manquantes extremement haut (supérieur à 95%) et leurs valeurs présentes ne sont pas exploitables.<br/>**à retirer**\n",
    "\n",
    "- *DefaultData* : <br/> Cette variable précise si au moins une des variables de l'individu a conservé des valeurs par default. Cela représente une mesure de fiabilité du relevé mais n'apporte pas d'informations utiles à notre projet. N'implique pas forcément une corruption de l'individu, pas de selection à effectuer sur ce critère<br/>**à retirer**\n",
    "\n",
    "- *DataYear* :<br/> Seules deux valeurs possibles pour cette variable : 2015 et 2016. Notre modélisation n'a pas de dimension temporelle : l'étude ne prends pas en compte les facteurs spécifiques à une année (météo).La variable ne sera pas utilisable.<br/> **à retirer**\n",
    "\n",
    "- *ComplianceStatus* :<br/> Représente si l'individu a atteint les objectifs demandé par l'analyse comparative du rendement énergétique, la variable est donc déduite de la consommation énergétique, il ne s'agit pas d'une donnée exploitable<br/>**à retirer**\n",
    "\n",
    "#### Conclusion de l'analyse :\n",
    "\n",
    "<p>Selon l'analyse les tâches suivantes seront à effectuées sur les variables:</p>\n",
    "\n",
    " **Retirer du dataset les variables:** *ComplianceStatus*, *DataYear*, *DefaultData*, *Outlier*, *Comments*, *SiteEUIWN(kBtu/sf)*, *SourceEUIWN(kBtu/sf)*, *SiteEnergyUseWN(kBtu)*, *SiteEUI(kBtu/sf)*, *ListOfAllPropertyUseTypes*, *GHGEmissionsIntensity*, *SourceEUI(kBtu/sf)*,*NaturalGas(therms)*, *Electricity(kWh)*, *SteamUse(kBtu)*, *NaturalGas(kBtu)*, *Electricity(kBtu)*, *YearsENERGYSTARCertified*, *ZipCode*, *State* et *City*\n",
    "\n",
    " **Reformater les variables suivantes :** *BuildingType*, *PrimaryPropertyType*, *ListOfAllPropertyUseTypes*, *LargestPropertyUseType*, *SecondLargestPropertyUseType* et *ThirdLargestPropertyUseType*\n",
    "\n",
    " **Créer une variable d'identification avec :** *OSEBuildingID*, *TaxParcelIdentificationNumber*, *PropertyName* et *Address*\n",
    " \n",
    " **Créer une variable de zone géographique avec :** *Neighborhood* et *CouncilDistrictCode*\n",
    "\n",
    " **Exprimer en ratio les variables de superficie :** *PropertyGFATotal*, *PropertyGFAParking*, *PropertyGFABuilding(s)*, *LargestPropertyUseTypeGFA*, *SecondLargestPropertyUseTypeGFA*, *ThirdLargestPropertyUseTypeGFA*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f3b741-d668-401f-b7f6-1b749ce98f18",
   "metadata": {},
   "source": [
    "Avant de supprimer les doublons, nous vérifions les différences entre 2016 et 2015.\n",
    "Nous observons : \n",
    "- Aucune différence dans l'année de construction\n",
    "- On garde les données les plus récentes (2016) pour les variables sauf si la valeurs de 2016 est à Nan, 0 ou 'Other':\n",
    "    - *BuildingType* , *PrimaryPropertyType*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1bc3d777-8898-4d06-99b2-a9ad1d483d5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3284, 1)\n"
     ]
    }
   ],
   "source": [
    "###################################################################################################\n",
    "####################                   Gestion des doublons                     ###################\n",
    "###################################################################################################\n",
    "\n",
    "t = ['ComplianceStatus', 'DataYear', 'DefaultData', 'Outlier', 'Comments',\n",
    "                          'SiteEUIWN(kBtu/sf)', 'SourceEUIWN(kBtu/sf)', 'SiteEnergyUseWN(kBtu)',\n",
    "                          'SiteEUI(kBtu/sf)', 'GHGEmissionsIntensity', 'SourceEUI(kBtu/sf)',\n",
    "                          'NaturalGas(therms)', 'Electricity(kWh)', 'SteamUse(kBtu)',\n",
    "                          'NaturalGas(kBtu)', 'Electricity(kBtu)', 'YearsENERGYSTARCertified',\n",
    "                          'ZipCode', 'State', 'City', 'ListOfAllPropertyUseTypes', 'DataYear','OSEBuildingID']\n",
    "\n",
    "duplicate = df.loc[df.duplicated('OSEBuildingID'),'OSEBuildingID'].to_frame()\n",
    "print(duplicate.shape)\n",
    "L = [x for x in df.columns if x not in t]\n",
    "for c in L :\n",
    "    var_2015 = []\n",
    "    var_2016 = []\n",
    "    for d in duplicate.values:\n",
    "        d = d[0]\n",
    "        var_2015.append(df.loc[((df['DataYear']==2015)&(df['OSEBuildingID'] == d)),c].values[0])\n",
    "        var_2016.append(df.loc[((df['DataYear']==2016)&(df['OSEBuildingID'] == d)),c].values[0])\n",
    "    duplicate[c+'_2015'] = var_2015\n",
    "    duplicate[c+'_2016'] = var_2016\n",
    "\n",
    "    \n",
    "    \n",
    "# 5. Reformating identification\n",
    "description = df.agg(lambda x: f\"{x['OSEBuildingID']} : {x['PropertyName']} at {x['Address']}\", axis=1)\n",
    "df.loc[:, \"Description\"] = description\n",
    "                                       \n",
    "df.drop_duplicates(subset=['OSEBuildingID'], inplace=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "befccd2e-9e80-4a19-b1c5-0ce0e7e1149e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#duplicate = duplicate.loc[duplicate.duplicated(['OSEBuildingID_2015','OSEBuildingID_2016'])]\n",
    "#duplicate.drop(['OSEBuildingID_2015','OSEBuildingID_2016'],inplace=True)\n",
    "for c in L :\n",
    "    #print(duplicate.loc[duplicate[c+'_2015']!=duplicate[c+'_2016']].shape[0],\" différences dans \",c)\n",
    "    pass#if duplicate.loc[duplicate[c+'_2015']==duplicate[c+'_2016']].shape[0] != duplicate.shape[0]:\n",
    "        #print(duplicate.loc[duplicate[c+'_2015']!=duplicate[c+'_2016'], [c+'_2015',c+'_2016']])\n",
    "    #print()\n",
    "test = df.loc[df.duplicated('OSEBuildingID',keep=False),['OSEBuildingID','Latitude','Longitude','ENERGYSTARScore','DataYear','PrimaryPropertyType']].dropna()\n",
    "alt.Chart(test).mark_bar().encode(\n",
    "    x='index',\n",
    "    y='ENERGYSTARScore:N',\n",
    "    color = 'DataYear:O'\n",
    ")\n",
    "alt.Chart(df.sample(500)).mark_circle(size=60).encode(\n",
    "    x='Latitude',\n",
    "    y='Longitude',\n",
    "    color='DataYear:O'\n",
    ").interactive()\n",
    "\n",
    "map_osm = folium.Map()\n",
    "map_osm = folium.Map(location=[df['Latitude'].mean(), df['Longitude'].mean()],zoom_start=11)\n",
    "for i in range(0,len(duplicate)):\n",
    "    folium.Circle([duplicate.iloc[i]['Latitude_2015'],duplicate.iloc[i]['Longitude_2015']], \n",
    "                      popup=duplicate.iloc[i]['Address_2015'], radius =100, color='red').add_to(map_osm)\n",
    "    folium.Circle([duplicate.iloc[i]['Latitude_2016'],duplicate.iloc[i]['Longitude_2016']], \n",
    "                      popup=duplicate.iloc[i]['Address_2016'], radius =100, color='blue').add_to(map_osm)\n",
    "duplicate.duplicated(subset=['Latitude_2016','Longitude_2016']).sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "10aa53f3-bba9-4daf-9777-ebf3d6acb16b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "###################################################################################################\n",
    "####################            Reformatage des zones géographiques             ###################\n",
    "###################################################################################################\n",
    "\n",
    "# 1. Standardisé les voisinages\n",
    "\n",
    "neighbor_names = df['Neighborhood'].apply(lambda x : x.upper())\n",
    "df.loc[:, 'Neighborhood'] = neighbor_names\n",
    "df['Neighborhood'].replace('DELRIDGE NEIGHBORHOODS', 'DELRIDGE', inplace=True)\n",
    "\n",
    "# 2. Visualiser les zones districts et voisinage\n",
    "map_n = folium.Map(location=[df['Latitude'].mean(), df['Longitude'].mean()],zoom_start=11)\n",
    "map_d = folium.Map(location=[df['Latitude'].mean(), df['Longitude'].mean()],zoom_start=11)\n",
    "change_color = list(df['Neighborhood'].unique())\n",
    "for i, val  in df.T.items():\n",
    "    index = change_color.index(val['Neighborhood'])\n",
    "    c = list_of_color[index]\n",
    "    folium.Circle([ val['Latitude'], val['Longitude'] ], popup=val['Description'],\n",
    "                  radius =100, color=c).add_to(map_n)\n",
    "map_n.save('image/Carte_neighborhood.html')\n",
    "\n",
    "change_color = list(df['CouncilDistrictCode'].unique())\n",
    "for i, val  in df.T.items():\n",
    "    index = change_color.index(val['CouncilDistrictCode'])\n",
    "    c = list_of_color[index]\n",
    "    folium.Circle([ val['Latitude'], val['Longitude'] ], popup=val['Description'],\n",
    "                  radius =100, color=c).add_to(map_d)\n",
    "map_d.save('image/Carte_district.html')\n",
    "\n",
    "# 3. Créer une nouvelle variable avec une granularité plus fine\n",
    "for i, val in df[['CouncilDistrictCode', 'Neighborhood']].T.items():\n",
    "    d = val['CouncilDistrictCode']\n",
    "    n = val['Neighborhood']\n",
    "    if n == 'DOWNTOWN':\n",
    "        if d == 2 or d == 3 :\n",
    "            df.loc[i,'Zone'] = 'South DownTown'\n",
    "        else:\n",
    "            df.loc[i,'Zone'] = 'North DownTown'\n",
    "    else:\n",
    "        df.loc[i,'Zone'] = n\n",
    "\n",
    "# 4. Retirer les variables \n",
    "        \n",
    "df.drop(['CouncilDistrictCode', 'Neighborhood'], axis=1, inplace=True)       \n",
    "        \n",
    "# 5. Visualiser la nouvelle variable\n",
    "map_z = folium.Map(location=[df['Latitude'].mean(), df['Longitude'].mean()],zoom_start=11)\n",
    "change_color = list(df['Zone'].unique())\n",
    "for i, val  in df.T.items():\n",
    "    index = change_color.index(val['Zone'])\n",
    "    c = list_of_color[index]\n",
    "    folium.Circle([ val['Latitude'], val['Longitude'] ], popup=val['Description'],\n",
    "                  radius =100, color=c).add_to(map_n)\n",
    "map_n.save('image/Carte_zones.html')\n",
    "\n",
    "\n",
    "# 6. Centrer les valeurs de latitude et longitude par rapport à Seattle\n",
    "\n",
    "seattle_lat = 47.6062095\n",
    "seattle_long = -122.3320708\n",
    "\n",
    "new_latitude = df['Latitude'].apply(lambda x : seattle_lat - x)\n",
    "new_longitude = df['Longitude'].apply(lambda x : seattle_long - x)\n",
    "df.loc[:, 'Latitude'] = new_latitude\n",
    "df.loc[:, 'Longitude'] = new_longitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7a0025d3-d398-4fbc-8860-0d2fde30a34e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "North DownTown           493\n",
       "EAST                     457\n",
       "MAGNOLIA / QUEEN ANNE    428\n",
       "GREATER DUWAMISH         379\n",
       "NORTHEAST                281\n",
       "LAKE UNION               258\n",
       "NORTHWEST                229\n",
       "NORTH                    191\n",
       "SOUTHWEST                167\n",
       "BALLARD                  136\n",
       "CENTRAL                  135\n",
       "SOUTHEAST                 98\n",
       "South DownTown            94\n",
       "DELRIDGE                  86\n",
       "Name: Zone, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Zone'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc01b815-01b4-43cf-a2ad-61864586e514",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################################################\n",
    "####################                   Reformating variables                    ###################\n",
    "###################################################################################################\n",
    "\n",
    "#Keeping a DataFrame with all informations, it could be usefull\n",
    "df = data_global\n",
    "\n",
    "#Reformate values spell check PrimaryPropertyType values - remove '/n' at the end\n",
    "type_names = df['PrimaryPropertyType'].apply(lambda x : x.replace('\\n',''))\n",
    "df.loc[:, 'PrimaryPropertyType'] = type_names \n",
    "\n",
    "df['LargestPropertyUseType'].fillna(df['PrimaryPropertyType'], inplace=True)\n",
    "df.loc[(df['LargestPropertyUseTypeGFA'].isna() & df['SecondLargestPropertyUseType'].isna()),\n",
    "       'LargestPropertyUseTypeGFA'] = df['PropertyGFABuilding(s)']\n",
    "df['LargestPropertyUseType'].replace(['Mixed Use Property'], ['Other'] ,inplace=True)\n",
    "df['ListOfAllPropertyUseTypes'].fillna(df['LargestPropertyUseType'],inplace=True)\n",
    "df['LargestPropertyUseType'].replace(rename_usetype.keys(),\n",
    "                                              rename_usetype.values(), inplace=True)\n",
    "df['SecondLargestPropertyUseType'].replace(rename_usetype.keys(),\n",
    "                                                    rename_usetype.values(), inplace=True)\n",
    "df['ThirdLargestPropertyUseType'].replace(rename_usetype.keys(),\n",
    "                                                   rename_usetype.values(), inplace=True)\n",
    "df['PrimaryPropertyType'].replace(rename_usetype.keys(),\n",
    "                                           rename_usetype.values(), inplace=True)\n",
    "\n",
    "#Remove parking from PropertyUsage - Check with 'PropertyGFAParking'\n",
    "#TODO\n",
    "\n",
    "\n",
    "#df.drop(['CouncilDistrictCode', 'Neighborhood'], axis=1, inplace=True)\n",
    "#Vérifier sur une carte la pertinence de ma notation\n",
    "#Reformating identification\n",
    "description = df.agg(lambda x: f\"{x['OSEBuildingID']} : {x['PropertyName']} at {x['Address']}\", axis=1)\n",
    "df.loc[:, \"Description\"] = description\n",
    "                                       \n",
    "df.drop_duplicates(subset=['OSEBuildingID'], inplace=True)                                       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9894759a-660e-4c9f-a982-e7705457aa1c",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Nettoyage des données :\n",
    "\n",
    "##### Supprimer les bâtiments résidentiels:\n",
    "\n",
    "<p>Dans l'intitulé du projet il est dit :</p>\n",
    "\n",
    ">Pour atteindre son objectif de ville neutre en émissions de carbone en 2050, votre équipe s’intéresse de près aux émissions **des bâtiments non destinés à l’habitation.**\n",
    "\n",
    "<p>Les bâtiments résidentiels doivent être supprimer de notre DataFrame. Les valeurs de *BuildingType* et *PrimaryPropertyType* sont étudiées, il s'y trouve trois valeurs représentant des bâtiment résidentiels :</p>\n",
    "\n",
    " - Multifamily MR (5-9)\n",
    " - Multifamily LR (1-4)\n",
    " - Multifamily HR (10+)\n",
    " - Low-Rise Multifamily\n",
    " \n",
    "<p>Tout individus ayant ces valeurs dans le champs *BuildingType* est donc retirer de notre DataFrame</p>\n",
    "\n",
    "<p>Dans le même temps, les variables :</p> \n",
    "\n",
    "- *BuildingType*, \n",
    "- *PrimaryPropertyType*,\n",
    "- *ListOfAllPropertyUseTypes*,\n",
    "- *LargestPropertyUseType*,\n",
    "- *SecondLargestPropertyUseType*,\n",
    "- *ThirdLargestPropertyUseType* \n",
    "\n",
    "<p>seront reformatées</p>\n",
    "\n",
    "En analysant de plus près les données de ces variables les faits suivants sont visibles :\n",
    "\n",
    "- *ListOfAllPropertyUseTypes*\n",
    "\n",
    "La liste des types d'usage *ListOfAllPropertyUseTypes* n'est pas ordonnée selon la hierarchie définie par *LargestPropertyUseType*, *SecondLargestPropertyUseType* et *ThirdLargestPropertyUseType*. La taille maximum de cette liste est de 13 mais seuls 8,6% des individus ont plus de 3 usages. Presque la moitié des individu (49,5%), n'ont qu'un seul et unique usage. Conserver les informations relatives aux usages après le troisième plus important est donc peu pertinent. *ListOfAllPropertyUseTypes* ne semble pas nécessaire et est donc supprimée.\n",
    "\n",
    "- Les parkings\n",
    "\n",
    "La taille des parking est déjà prise en compte dans la variable *GFAParking*, il est donc redondant de retrouver 'parking' dans la liste des usages du bâtiment. Nous allons retirer cette usage de nos listes lorsqu'on décompte plus d'un seul usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c90a529a-7bf8-4223-a7b8-a0df8dd9aa30",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################################################\n",
    "####################                   Réduction des individus                  ###################\n",
    "###################################################################################################\n",
    "\n",
    "# Removing residentials buildings from dataset\n",
    "resi_buildings = ['Multifamily MR (5-9)', 'Multifamily LR (1-4)', 'Multifamily HR (10+)']\n",
    "df = df.loc[~df['BuildingType'].isin(resi_buildings)]\n",
    "df = df.loc[df['PrimaryPropertyType'] != 'Low-Rise Multifamily']\n",
    "\n",
    "# Removing individual without energy use or GHG emissions count\n",
    "# is it needed ?\n",
    "df = df.loc[~df['TotalGHGEmissions'].isna()]\n",
    "df = df.loc[~df['SiteEnergyUse(kBtu)'].isna()]\n",
    "\n",
    "\n",
    "# Removing duplicates from dataset\n",
    "# Etudier les duplicate afin de montrer l'intérêt \n",
    "#d'une étude temporelle prenant en compte les années.\n",
    "# Pour le moment soyons basique :\n",
    "df.drop_duplicates(subset=['OSEBuildingID'], inplace=True)\n",
    "#Print les écarts entre 2016 et 2015 ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae9d9054-6a71-4e86-8d77-f1ee4d7b54e3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['CouncilDistrictCode', 'Neighborhood'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[1;32mIn [9]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m#These variables have been reformated :\u001b[39;00m\n\u001b[0;32m     14\u001b[0m variables_indésirables\u001b[38;5;241m.\u001b[39mextend([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAddress\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOSEBuildingID\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPropertyName\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCouncilDistrictCode\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     15\u001b[0m                                \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNeighborhood\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTaxParcelIdentificationNumber\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m---> 17\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvariables_indésirables\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\util\\_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[0;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[0;32m    310\u001b[0m     )\n\u001b[1;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\frame.py:4956\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4808\u001b[0m \u001b[38;5;129m@deprecate_nonkeyword_arguments\u001b[39m(version\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, allowed_args\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m   4809\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdrop\u001b[39m(\n\u001b[0;32m   4810\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4817\u001b[0m     errors: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   4818\u001b[0m ):\n\u001b[0;32m   4819\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4820\u001b[0m \u001b[38;5;124;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[0;32m   4821\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4954\u001b[0m \u001b[38;5;124;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[0;32m   4955\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4956\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4957\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4958\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4959\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4960\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4961\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4962\u001b[0m \u001b[43m        \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4963\u001b[0m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4964\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\generic.py:4279\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4277\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m   4278\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 4279\u001b[0m         obj \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_drop_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4281\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[0;32m   4282\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_inplace(obj)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\generic.py:4323\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[1;34m(self, labels, axis, level, errors, consolidate, only_slice)\u001b[0m\n\u001b[0;32m   4321\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m   4322\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 4323\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m \u001b[43maxis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4324\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mget_indexer(new_axis)\n\u001b[0;32m   4326\u001b[0m \u001b[38;5;66;03m# Case for non-unique axis\u001b[39;00m\n\u001b[0;32m   4327\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\indexes\\base.py:6644\u001b[0m, in \u001b[0;36mIndex.drop\u001b[1;34m(self, labels, errors)\u001b[0m\n\u001b[0;32m   6642\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[0;32m   6643\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 6644\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(labels[mask])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in axis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6645\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m indexer[\u001b[38;5;241m~\u001b[39mmask]\n\u001b[0;32m   6646\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelete(indexer)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['CouncilDistrictCode', 'Neighborhood'] not found in axis\""
     ]
    }
   ],
   "source": [
    "###################################################################################################\n",
    "####################                   Reduction des variables                  ###################\n",
    "###################################################################################################\n",
    "\n",
    "# Removing variable from dataset\n",
    "variables_indésirables = ['ComplianceStatus', 'DataYear', 'DefaultData', 'Outlier', 'Comments',\n",
    "                          'SiteEUIWN(kBtu/sf)', 'SourceEUIWN(kBtu/sf)', 'SiteEnergyUseWN(kBtu)',\n",
    "                          'SiteEUI(kBtu/sf)', 'GHGEmissionsIntensity', 'SourceEUI(kBtu/sf)',\n",
    "                          'NaturalGas(therms)', 'Electricity(kWh)', 'SteamUse(kBtu)',\n",
    "                          'NaturalGas(kBtu)', 'Electricity(kBtu)', 'YearsENERGYSTARCertified',\n",
    "                          'ZipCode', 'State', 'City', 'ListOfAllPropertyUseTypes']\n",
    "\n",
    "#These variables have been reformated :\n",
    "variables_indésirables.extend(['Address','OSEBuildingID', 'PropertyName', 'CouncilDistrictCode',\n",
    "                               'Neighborhood', 'TaxParcelIdentificationNumber'])\n",
    "\n",
    "df.drop(variables_indésirables, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6447ce69-bbbb-4984-a78a-2a11777f8a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################################################\n",
    "####################       Recherche d'erreurs et de valeurs aberrantes         ###################\n",
    "###################################################################################################\n",
    "\n",
    "\n",
    "#valeur outlier : row 6614 l'université de washington 111 bâtiments différents,\n",
    "#une années de construction trouble ( 1861 selon wiki, 1900 selon le dataset)\n",
    "df.drop(6614, inplace=True)\n",
    "\n",
    "df.loc[1371,'NumberofFloors'] = 2 #erreur CHINESE BAPTIST CHURCH don't have 99 floors but only 2\n",
    "\n",
    "#corrige les erreurs basique de GFA\n",
    "for i, gfa in df[[x for x in df.columns if 'GFA' in x]].copy().T.items():\n",
    "    for c, val in gfa.items() :\n",
    "        if val < 0 and val > -10 :\n",
    "            df.loc[i,c] = 0\n",
    "        elif val < 0 :\n",
    "            df.loc[i,c] = val*-1\n",
    "    if gfa['PropertyGFATotal'] != gfa['PropertyGFAParking'] + gfa['PropertyGFABuilding(s)'] :\n",
    "        if gfa['PropertyGFAParking'] ==  gfa['PropertyGFATotal'] + gfa['PropertyGFABuilding(s)']:\n",
    "            df.loc[i, 'PropertyGFAParking'] = gfa['PropertyGFATotal']\n",
    "            df.loc[i, 'PropertyGFATotal'] = gfa['PropertyGFAParking']\n",
    "        else:\n",
    "            df.loc[i, 'PropertyGFATotal'] = gfa['PropertyGFAParking']+gfa['PropertyGFABuilding(s)']\n",
    "            \n",
    "            \n",
    "print(data_global['ListOfAllPropertyUseTypes'].loc[data_global['PropertyGFAParking']==-2])\n",
    "deja_vu = ['BuildingType', 'PrimaryPropertyType', 'YearBuilt', 'NumberofBuildings',\n",
    "          'NumberofFloors', 'PropertyGFATotal', 'PropertyGFABuilding(s)', 'PropertyGFAParking',\n",
    "          ]\n",
    "for c in df.columns:\n",
    "    if c not in deja_vu :\n",
    "        print(df[c].describe())\n",
    "        print(df[c].isna().sum())\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b067cc6-3231-4a01-9b67-f7c9e07c4c91",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Nettoyage des données :\n",
    "\n",
    "##### Supprimer les doublons:\n",
    " \n",
    " **Supprimer les doublons de bâtiments** et choisir quelles valeurs conserver\n",
    " \n",
    " **Supprimer les individus sans informations** s'ils existent\n",
    " \n",
    " **Rechercher les valeurs aberrantes et les corriger**\n",
    " \n",
    " **Traiter les valeurs NaN**\n",
    "\n",
    "Nous allons regrouper les données d'identification des bâtiments afin qu'elles ne perturbent pas nos modélisations mais restent utilisables.\n",
    "\n",
    "identification_features = ['OSEBuildingID', 'PropertyName', 'Address', 'ZipCode']\n",
    "data_identification = data[identification_features]\n",
    "data.drop(identification_features, axis=1, inplace = True)\n",
    "\n",
    "\n",
    "Le but de notre programme est de supprimer les relevés couteux pour les années à venir. Nous allons donc exclure toutes les données de relève de notre dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471920d6-952a-45f7-bee6-91aa74e25044",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Doublons\n",
    "df.drop_duplicates(subset=['OSEBuildingID'], inplace=True)\n",
    "list_doublons = []\n",
    "for i, row in df.T.items():\n",
    "    if i not in big_set :\n",
    "        big_set = set()\n",
    "        address = row['Address']\n",
    "        Bldg_ID = row['OSEBuildingID']\n",
    "        Owner_ID = row['TaxParcelIdentificationNumber']\n",
    "        name = row['PropertyName']\n",
    "        lat = row['Latitude']\n",
    "        long = row['Longitude']\n",
    "        same_loc = df.loc[(df['Latitude'] == lat) & (df['Longitude'] == long) & (df['TaxParcelIdentificationNumber'] == Owner_ID)]# & (df['Address'] == address)]\n",
    "        if same_loc.shape[0] > 1 :\n",
    "            big_set.update(same_loc.index.values)\n",
    "        if len(list_doublons) == 0 and len(big_set) != 0 :\n",
    "            list_doublons.append(big_set)\n",
    "        else :\n",
    "            for x in list_doublons.copy():\n",
    "                if big_set.intersection(x):\n",
    "                    #if some elements are intercorrelated, merge the group\n",
    "                    list_doublons[list_doublons.index(x)] = set.union(x, big_set)\n",
    "                    big_set = set() # avoid adding duplicate\n",
    "                    break\n",
    "            if len(big_set) != 0 :\n",
    "                # add a new correlated group only if no intersection \n",
    "                list_doublons.append(big_set)\n",
    "\n",
    "for doubles in list_doublons:\n",
    "    for id_d in list(doubles):\n",
    "        print(df.loc[id_d,'PropertyName'], \" and \", df.loc[id_d,'DataYear'])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045a9bbb-9a89-4f18-a958-e07db7b63e70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "###################################################################################################\n",
    "####################                   Traitement des données                   ###################\n",
    "###################################################################################################\n",
    "\n",
    "# Traitement des données 1 : Removing residential building from dataset\n",
    "resi_buildings = ['Multifamily MR (5-9)', 'Multifamily LR (1-4)', 'Multifamily HR (10+)']\n",
    "df = df.loc[~df['BuildingType'].isin(resi_buildings)]\n",
    "\n",
    "\n",
    "for column in df.columns:\n",
    "    if df[column].nunique()<20:\n",
    "        print('Colonne {}, valeurs uniques :\\n{}\\n'.format(column, df[column].unique()))\n",
    "    else:\n",
    "        print('Colonne {}, {} valeurs uniques'.format(column, df[column].nunique()))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54279b6-35e2-4598-aa22-3a479d3a837b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beaf21d9-df5d-4757-8345-758137273333",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
